\documentclass{article}

\parindent=.25in
\parskip=2ex

\usepackage{amsthm}
\usepackage{amsbsy}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[dvips]{lscape}

\input{lauracode}
\input{lauracodePHD}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{notation}[theorem]{Notation}

\title{An introduction to the Concentration of Measure and Empirical Process Theory}
\author{Peadar Coyle}

\begin{document}

\renewcommand{\baselinestretch}{1.5}

\maketitle
\section{Introduction}
Concentration inequalities bound tail probabilities of general functions of independent
   random variables. There are several methods which are known to prove such inequalities,
   including martingale methods and Talagrand's induction method. 
      
 A novel way of deriving powerful inequalities, the \textit{entropy method}, based on
 logarithmic Sobolev inequalities, was developed by Ledoux, Bobkov, Massart, Rio,
Bousquet for proving sharp concentration bounds for maxima of empirical processes.
While it is true that empiricial process theory is used a 
lot in modern Machine Learning applications, it will take us too far a field to delve into all these details.
The thesis will start with an introduction to some classical Concentration of Measure inequalities. The area of
information theory will be mentioned \textit{en passant} but this is not a paper on information theory. 
Of chief interest for us is to provide applications from the world of Machine Learning or Statistics, but in a
mathematical context. 
\subsection{Concentration of Measure on the Sphere}
Measure concentration is a fairly general phenomenon, which asserts that a \textit{reasonable}
function $f: X \mapsto \mathbb{R}$ defined on a "large" probability space X "almost 
always" takes values that are "very close" to the average value of f on X.
\begin{example}
Let $\mathbb{R}^{n}$ be the n-dimensional Euclidean space of all n-tuples 
$x = (\xi_1, \cdots, \xi_n)$ with the standard scalar product and norm.
Let 
$\mathbb{S}^{n-1} = \lbrace x \in \mathbb{R}^{n} : \|x\| = 1 \rbrace $
be the unit sphere. We introduce the geodesic metric on the sphere 
$$dist(x,y) = \arccos <x, y>$$,
the distance between x and y being the angle between x and y and the rotation invariant
probability measure $\mu$ (which we will assume the existence and uniqueness of without a cumbersome
formal proof).
  Let $f: \mathbb{S}^{n-1} \mapsto \mathbb{R}$ be a function which is 1-Lipschitz:
  $\|f(x) - f(y)\| \leq \mathrm{dist}(x,y) \; \mathrm{for all} \; x,y \in \mathbb{S}^{n-1}$.
 Let $m_f$ be the median of f, that is the number such that
 $\mu\lbrace x: f(x) \geq m_{f} \rbrace \geq \dfrac{1}{2}$ and  $\mu\lbrace x: f(x) \leq m_{f} \rbrace \geq \dfrac{1}{2}$.
 We assume that for a Lipschitz function f, the median $m_f$ exists and is unique.
 Then, for any $\epsilon \gt 0$, 
$$\mu \lbrace x \in \mathbb{S}^{n-1} : \|f(x) - m_{f}\| \leq \epsilon \rbrace \geq 1 - \sqrt{\dfrac{\pi}{2}} e ^{\epsilon^{2} (n-2)/2}$$
In particular, the value of f(x) in a \textit{"typical"} point $x \in \mathbb{S}^{n-1}$ deviates from the median only by about
$1/{\sqrt{n}}$: that is, if $\epsilon_{n} \sqrt{n} \rightarrow +\infty$, however slowly, the probability that f(x) does not deviate from $m_f$ by more than $\epsilon_{n}$ tends to 1. 
\end{example}

\section{The big picture}
Roughly speaking, one says that a probability measure is \emph{concentrated} if any set of positive probability can be expanded very slightly to contain most of the probability. A classic example (which I learned in statistical mechanics) is the uniform distribution on the interior of unit spheres in high dimensions. A little bit of calculus shows that, as the number of dimensions grows, the ratio of the surface area to the volume does too, so a shell of thin, constant thickness on the interior of the sphere ends up containing almost all of its volume, and overlaps heavily with any set of positive probability. 
Equivalently, all well-behaved functional of a concentrated measure are very close to their expectation values with very high probability. (This is equivalent because one can specify a probability measure either through giving the probabilities of sets in the sigma-field, or by giving the expectation values of a sufficiently rich set of test functions, e.g., all bounded and continuous functions. 
[At least, bounded and continuous functions are enough for the weak topology on measures, but if you understand that qualification then you know what I'm saying anyway.])

In applications to probability, one is typically concerned with "concentration bounds", which are statements of the order of "for samples of size n, the probability that any function f in a well-behaved class F differs from its expectation value by h or more is at most $Ce^{-nr(h)}", with explicit values of the prefactor C and the rate function r; these might depend on the true distribution and on the function class, but not on the specific function f.

These finite-sample upper bounds are to be distinguished from asymptotic results giving matching upper and lower bounds (large deviations theory proper). They are also distinct from deviation inequalities which may depend on the specific function f, though obviously finding such deviation inequalities is often a first step to proving concentration-of-measure results.  
\subsection{The entropy method}
One of the more recent methods used to derive concentration inequalities, the so-called \emph{entropy method}, is rooted in the 
early investigations of Boltzmann and Gibbs into the foundations of statistical mechanics. A general problem of statistical mechanics
is to demonstrate the "equivalence of ensembles", which can be interpreted as an exponential concentration property of the Hamiltonian, or energy function. The history of the modern entropy method is long and convoluted - and takes us into quantum filed theory and the logarithmic Sobolev inequality of Leonard Gross, its hidden simplicity was understood and emphasized by Michel Ledoux.
Michel recognized the key role that subadditivity of entropy can play in the derivation of concentration inequalities. 
\section{Notation}
We begin by introduction some notation that is used throughout the paper. We assume
that $X_1, \cdots , X_n$ are independent random variables taking values in a
measurable space $\mathcal{X}$. Denote by $X_1^n$ the vector of these n random
variables. Let $f: \mathcal{X}^n \rightarrow \mathbb{R} $be some measurable function.
We are concerned with concentration of the random variable. 
\begin{equation*}
Z = f(X_1, \cdots X_n) \end{equation*}
	 Throughout, $X_1^{'} \cdots X_n^{'}$ denote independent copies of $X_1, \cdots X_n$
	 \begin{equation*}
\end{equation*}and we write \begin{equation*} Z^{(i)} = f\left( X_1, \cdots X_{i-1}
, X^{'}_i, \cdots X_n\right)
\end{equation*}
One of the first concentration inequalities was proved by Efron and Stein, and
further improved by Steele:
\begin{proposition}[Efron-Stein inequality]
Var(Z) \lt \dfrac{1}{2} \mathbb{E} \left[ \sum_{i=1}^{n} \left( 
Z-Z^{(i)}\right)^2\right]
\end{proposition}
The quantity $\sum_{i=1}^{n} \left( 
Z-Z^{(i)}\right)^2$ is called a jackknife estimate of variance. 
Note that the inequality becomes an equality if f is the sum of its components. 
The upshot of this result is that we have a simple and often sharp[TK: define] way
of bound the variance of complicated functions of independent random variables. 
While extremely useful, this result fails to capture the exponential nature 
of tails, present under many circumstances.
Our first result is that if the random variables $V_+$ and $V_-$ behave nicely in the
sense that their moment generating function[tk:to define] can be controlled, then,
indeed, we may obtain an exponential version of the Efron-Stein inequality:
\begin{theorem} For all $\theta \gt 0$ and $\lambda \in (0,1/\theta)$,
$$\log \Ebb[\exp(\lambda(Z- \Ebb[Z])]\leq \dfrac{\lambda \theta}{1- \lambda \theta} \log \Ebb[\exp()]$$
\end{theorem}
\begin{equation*}
\end{equation*}\begin{equation*}
\end{equation*}
Log-Sobolev inequalities, introduced by Leonard Gross in 1975 [22], are one of the
essential tools for proving concentration phenomena, not only because they require
in some sense less understanding about the underlying geometry of the measured
space, but also because they yield sharper results for concentration, i.e., Gaussian
rather than exponential. They are particularly well-suited for inﬁnite-dimensional
analysis.
We start by recalling the classical Sobolev embedding for functions in $R^{n}$
. Let
$\Omega$ be a bounded open domain in $R^n$ with sufficiently regular boundary (Lipschitz
boundary, or satisfying the ’cone’ condition for Brownian motion). Let Wk,p(Ω)
be the Sobolev space of order (k, p) where $k \geq  1$ and p $
in[1, \infty)$: this is the set
of functions for which all k (weak) derivatives are in $L^{p}
(\Omega)$. In particular, $W^{1,2}$
is denoted by H1
is a Hilbert space. Then the Sobolev embedding states the
following:
Perhaps the best-known log-Sobolev inequality, first explicitly referred to as such by Leonard Gross \cite{Gross}, pertains to the standard Gaussian distribution in Euclidean space $\reals^n$, and bounds the relative entropy $D(P \| G_n)$ between an arbitrary probability distribution $P$ on $\reals^n$ and the standard Gaussian measure $G_n$ by an ``energy-like'' quantity related to the squared norm of the gradient of the density of $P$ w.r.t.\ $G_n$ (here, it can be assumed without loss of generality that $P$ is absolutely continuous w.r.t.\ $G_n$, for otherwise both sides of the log-Sobolev inequality are equal to $+\infty$). By a clever analytic argument which he attributed to an unpublished note by Ira Herbst, Gross has used his log-Sobolev inequality to show that the logarithmic MGF $\Lambda(\lambda) = \ln \expectation[\exp(\lambda U)]$ of $U = f(X^n)$, where $X^n \sim G_n$ and $f : \reals^n \to \reals$ is any sufficiently smooth function with $\| \nabla f \| \le 1$, can be bounded as $\Lambda(\lambda) \le \lambda^2/2$. This bound then yields the optimal Gaussian concentration inequality $\pr\left(\left|f(X^n)-\expectation[f(X^n)]\right| > t\right) \le 2\exp\left(-t^2/2\right)$ for $X^n \sim G_n$. (It should be pointed out that the Gaussian log-Sobolev inequality has a curious history, and seems to have been discovered independently in various equivalent forms by several people, e.g., by Stam \cite{Stam} in the context of information theory, and by Federbush \cite{Federbush} in the context of mathematical quantum field theory. Through the work of Stam \cite{Stam}, the Gaussian log-Sobolev inequality has been linked to several other information-theoretic notions, such as concavity of entropy power \cite{Dembo_Cover_Thomas, Villani_EP_concavity,Toscani}.)

In a nutshell, the entropy method takes this idea and applies it beyond the Gaussian case. In abstract terms, log-Sobolev inequalities are functional inequalities that relate the relative entropy between an arbitrary distribution $Q$ w.r.t.\ #the distribution $P$ of interest to some ``energy functional'' 
of the density $f = \d Q / \d P$. If one is interested in studying concentration properties of some function $U = f(Z)$ with $Z \sim P$, the core of the entropy method consists in applying an appropriate log-Sobolev inequality to the {\em tilted distributions} $P^{(\lambda f)}$ with $\d P^{(\lambda f)}/\d P \propto \exp(\lambda f)$. 
Provided the function $f$ is well-behaved in the sense of having bounded ``energy,'' one uses the ``Herbst argument'' to pass from the log-Sobolev 
inequality to the bound $\ln \expectation[\exp(\lambda U)] \le c\lambda^2/(2C)$, where $c > 0$ depends only on the distribution $P$, while $C > 0$ is 
determined by the energy content of $f$. While there is no general technique for deriving log-Sobolev inequalities, there are nevertheless some 
underlying principles that can be exploited for that purpose. We discuss some of these principles in Chapter~\ref{chapter: entropy method}. More 
information on log-Sobolev inequalities can be found in several excellent monographs and lecture notes \cite{Ledoux,Massart_book,Guionnet_Zegarlinski,
Ledoux_lecture_notes,Royer}, as well as in \cite{Bobkov_Gotze_expint,Bobkov_Ledoux,Bobkov_Tetali,Chafai,KitsosT_IT09} and references therein.



Around the same time as Ledoux first introduced the entropy method in \cite{Ledoux_paper}, Katalin Marton has shown in a breakthrough paper \cite{
Marton_dbar}  that to prove concentration bounds one can bypass functional inequalities and work directly on the level of probability measures. More 
specifically, Marton has shown that Gaussian concentration bounds can be deduced from so-called {\em transportation-cost inequalities}. These 
inequalities, discussed in detail in Section~\ref{sec:transportation}, relate information-theoretic quantities, 
such as the relative entropy, to a certain class of distances between probability measures on the metric space where the random variables of interest 
are defined. These so-called {\em Wasserstein distances} have been the subject of intense research activity that touches upon probability theory, 
functional analysis, dynamical systems and partial differential equations, statistical physics, and differential geometry. A great deal of information 
on this field of {\em optimal transportation} can be found in two books by C\'edric Villani --- \cite{Villani_TOT} offers a concise and fairly 
elementary introduction, while a more recent monograph \cite{Villani_newbook} is a lot more detailed and encyclopedic. Multiple connections between 
optimal transportation, concentration of measure, and information theory are also explored in \cite{Gozlan_Leonard,Dembo,Cattiaux_Guillin,
Dembo_Zeitouni_TC,Djellout_Guillin_Wu,Gozlan,E_Milman}. (We also note that Wasserstein distances have been used in information theory in the context of 
lossy source coding \cite{Gray_Neuhoff_Shields_dbar,Gray_Neuhoff_Omura}.)
\section{Basic concentration inequalities via the martingale approach}
\label{section: Two Basic Concentration Inequalities}
In the following section, some basic inequalities that are widely
used for proving concentration inequalities are presented, whose
derivation relies on the martingale approach. Their proofs convey
the main concepts of the martingale approach for proving concentration.
Their presentation also motivates some further refinements that are
considered in the continuation of this chapter.

\subsection{The Azuma-Hoeffding inequality}
\label{subsection: Azuma's inequality}
The Azuma-Hoeffding inequality\footnote{The
Azuma-Hoeffding inequality is also known as Azuma's inequality.
Since it is referred numerous times in this chapter, it will be
named Azuma's inequality for the sake of
brevity.} is a useful concentration inequality for
bounded-difference martingales. It was proved in \cite{Hoeffding}
for independent bounded random variables, followed by a discussion
on sums of dependent random variables; this inequality was later
derived in \cite{Azuma} for the more general setting of
bounded-difference martingales. In the following, this inequality
is introduced.

\begin{theorem}{\bf[Azuma-Hoeffding inequality]}
Let $\{X_k, \mathcal{F}_k\}_{k=0}^n$
be a discrete-parameter real-valued martingale sequence.
Suppose that, for every $k \in \{1, \ldots, n\}$, the condition
$ |X_k - X_{k-1}| \leq d_k$
holds a.s. for a real-valued sequence $\{d_k\}_{k=1}^n$ of
non-negative numbers. Then, for every $\alpha > 0$,
\begin{equation}
\pr( | X_n - X_0 | \geq \alpha) \leq 2 \exp\left(-\frac{\alpha^2}{2
\sum_{k=1}^n d_k^2}\right).
\label{eq: Azuma's concentration inequality - general case}
\end{equation}
\label{theorem: Azuma's concentration inequality}
\end{theorem}

It is noted that \eqref{eq: Azuma's concentration inequality - general case}
is typically interpreted as $\frac{X_n - X_0}{\sqrt{n}}$ being sub-Gaussian.
The proof of the Azuma-Hoeffding inequality serves also
to present the basic principles on which the martingale
approach for proving concentration results is based.
Therefore, we present in the following the proof of this
inequality.

\begin{proof}
For an arbitrary $\alpha > 0$,
\begin{equation}
\pr(|X_n - X_0| \geq \alpha) = \pr(X_n - X_0 \geq \alpha) + \pr(X_n - X_0 \leq -\alpha).
\label{eq: union of disjoint events}
\end{equation}
Let $\xi_i \triangleq X_i - X_{i-1}$ for $i=1, \ldots, n$ designate
the jumps of the martingale sequence. Then, it follows by assumption that
$|\xi_k| \leq d_k$ and $\expectation[\xi_k \, | \, \mathcal{F}_{k-1}] = 0$
a.s. for every $k \in \{1, \ldots, n\}$.

From Chernoff's inequality,
\begin{eqnarray}
&& \pr(X_n - X_0 \geq \alpha) \nonumber \\
&& = \pr \Biggl(\sum_{i=1}^n \xi_i \geq \alpha \Biggr) \nonumber \\
&& \leq e^{-\alpha t} \, \expectation\left[\exp \left(t \sum_{i=1}^n \xi_i \right) \right],
\quad \forall \, t \geq 0.
\label{eq: Chernoff's inequality}
\end{eqnarray}
Furthermore,
\begin{eqnarray}
&& \expectation \biggl[ \exp \biggl(t \sum_{k=1}^n \xi_k \biggr)
\biggr] \nonumber \\
&& = \expectation \Biggl[ \expectation \biggl[ \exp \biggl(t
\sum_{k=1}^n \xi_k \biggr) \, | \, \mathcal{F}_{n-1} \biggr] \Biggr]
\nonumber \\
&& = \expectation \Biggl[ \exp \biggl(t \sum_{k=1}^{n-1} \xi_k
\biggr) \, \expectation \bigl[ \exp(t \xi_n) \, | \,
\mathcal{F}_{n-1} \bigr] \Biggr]
\label{eq: smoothing theorem}
\end{eqnarray}
where the last equality holds since $Y \triangleq \exp \bigl(t
\sum_{k=1}^{n-1} \xi_k \bigr)$ is $\mathcal{F}_{n-1}$-measurable;
this holds due to fact that $\xi_k \triangleq X_k
- X_{k-1}$ is $\mathcal{F}_k$-measurable for every $k \in
\naturals$, and $\mathcal{F}_k \subseteq \mathcal{F}_{n-1}$ for $0
\leq k \leq n-1$ since $\{\mathcal{F}_k\}_{k=0}^{n}$ is a
filtration. Hence, the RV $\sum_{k=1}^{n-1} \xi_k$ and $Y$
are both $\mathcal{F}_{n-1}$-measurable, and
$\expectation[ XY | \mathcal{F}_{n-1}] =
Y \, \expectation[ X | \mathcal{F}_{n-1}].$

Due to the convexity of the exponential function, the straight line connecting
the end points of the function over the interval $[-d_k, d_k]$ lies
above this function. Since $|\xi_k| \leq d_k$ for every $k$
(note that $\expectation[\xi_k \, | \, \mathcal{F}_{k-1}] = 0$), it follows that
\begin{eqnarray}
&& \expectation \bigl[e^{t \xi_k} \, | \, \mathcal{F}_{k-1}\bigr] \nonumber \\
&& \leq \expectation \Bigl[\frac{(d_k + \xi_k) e^{t d_k} + (d_k-\xi_k) e^{-t d_k}}{2 d_k}
\, | \, \mathcal{F}_{k-1} \Bigr] \nonumber \\
&& = \frac{1}{2} \, \bigl(e^{t d_k} + e^{-t d_k} \bigr) \nonumber \\
&& = \cosh(t d_k).
\label{eq: cosine hyperbolic}
\end{eqnarray}
Since, for every integer $m \geq 0$,
$$(2m)! \geq (2m) (2m-2) \ldots 2 = 2^m \, m!$$
then, due to the power series expansions of the hyperbolic cosine and exponential functions,
$$\cosh(t d_k) = \sum_{m=0}^{\infty} \frac{(t d_k)^{2m}}{(2m)!}
\leq \sum_{m=0}^{\infty} \frac{(t d_k)^{2m}}{2^m \, m!} = e^{\frac{t^2 \, d_k^2}{2}}$$
which therefore implies that
$$ \expectation \bigl[e^{t \xi_k} \, | \, \mathcal{F}_{k-1}\bigr] \leq e^{\frac{t^2 \, d_k^2}{2}}.$$
Consequently, by repeatedly using the recursion in \eqref{eq: smoothing theorem}, it follows
that
\begin{equation*}
\expectation \biggl[ \exp \biggl(t \sum_{k=1}^n \xi_k \biggr)
\biggr] \leq \prod_{k=1}^n \exp\left(\frac{t^2 \, d_k^2}{2}\right)
= \exp \left(\frac{t^2}{2} \, \sum_{k=1}^n d_k^2 \right)
\end{equation*}
which then gives (see \eqref{eq: Chernoff's inequality}) that
\begin{equation*}
\pr(X_n - X_0 \geq \alpha) \leq
\exp\left(-\alpha t + \frac{t^2}{2} \, \sum_{k=1}^n d_k^2 \right), \quad \forall \, t \geq 0.
\end{equation*}
An optimization over the free parameter $t \geq 0$ gives that
$t = \alpha \left(\sum_{k=1}^n d_k^2\right)^{-1}$, and
\begin{equation}
\pr(X_n - X_0 \geq \alpha) \leq \exp \left(-\frac{\alpha^2}{2 \sum_{k=1}^n d_k^2} \right).
\label{eq: one-sided Azuma's inequality}
\end{equation}
Since, by assumption, $\{X_k, \mathcal{F}_k \}$ is a martingale with bounded jumps,
so is $\{-X_k, \mathcal{F}_{k}\}$ (with the same bounds on its jumps). This implies
that the same bound is also valid for the probability $\pr(X_n - X_0 \leq -\alpha)$
and together with \eqref{eq: union of disjoint events} it completes the proof of
Theorem~\ref{theorem: Azuma's concentration inequality}.
\end{proof}

The proof of this inequality will be revisited later
in this chapter for the derivation of some refined versions,
whose use and advantage will be also exemplified.

\begin{remark}
In \cite[Theorem~3.13]{McDiarmid_tutorial}, Azuma's inequality is
stated as follows: Let $\{Y_k, \mathcal{F}_k\}_{k=0}^n$ be a
martingale-difference sequence with $Y_0=0$ (i.e., $Y_k$ is
$\mathcal{F}_k$-measurable, $\expectation[|Y_k|] < \infty$ and
$\expectation[Y_k| \mathcal{F}_{k-1}]=0$ a.s. for every $k \in
\{1, \ldots, n\}$). Assume that, for every $k$, there
exist some numbers $a_k, b_k \in \reals$ such that a.s.
$a_k \leq Y_k \leq b_k$. Then, for every $r \geq 0$,
\begin{equation}
\pr \left( \bigg|\sum_{k=1}^n Y_k\bigg| \geq r \right) \leq 2
\exp\left(-\frac{2r^2}{\sum_{k=1}^n (b_k-a_k)^2} \right).
\label{eq: concentration inequality for a martingale-difference
sequence (McDiarmid's tutorial)}
\end{equation}
As a consequence of this inequality, consider a discrete-parameter
real-valued martingale sequence $\{X_k, \mathcal{F}_k\}_{k=0}^n$
where $a_k \leq X_k - X_{k-1} \leq b_k$ a.s. for every $k$. Let $Y_k
\triangleq X_k - X_{k-1}$ for every $k \in \{1, \ldots, n\}$, so
since $\{Y_k, \mathcal{F}_k\}_{k=0}^n$ is a
martingale-difference sequence and $\sum_{k=1}^n Y_k = X_n - X_0$, then
\begin{equation}
\pr \left( |X_n - X_0| \geq r \right) \leq 2
\exp\left(-\frac{2r^2}{\sum_{k=1}^n (b_k-a_k)^2} \right), \quad \forall \, r > 0.
\label{eq: concentration inequality for a martingale sequence
(McDiarmid's tutorial)}
\end{equation}
\end{remark}

\begin{example}
Let $\{Y_i\}_{i=0}^{\infty}$ be i.i.d. binary random variables
which get the values $\pm d$, for some constant $d>0$, with equal
probability. Let $X_k = \sum_{i=0}^k Y_i$ for $k \in \{0, 1,
\ldots, \}$, and define the natural filtration $\mathcal{F}_0
\subseteq \mathcal{F}_1 \subseteq \mathcal{F}_2 \ldots $ where
$$\mathcal{F}_k = \sigma(Y_0, \ldots, Y_k) \, , \quad \forall
\, k \in \{0, 1, \ldots, \}$$ is the $\sigma$-algebra that is
generated by the random variables $Y_0, \ldots, Y_k$. Note that
$\{X_k, \mathcal{F}_k\}_{k=0}^{\infty}$ is a martingale sequence, and
(a.s.) $ |X_k - X_{k-1}| = |Y_k| = d, \, \forall \, k \in
\naturals$. It therefore follows from Azuma's inequality that
\begin{equation}
\pr( | X_n - X_0 | \geq \alpha \sqrt{n}) \leq 2
\exp\left(-\frac{\alpha^2}{2d^2}\right). \label{eq: Azuma's
inequality for example1}
\end{equation}
for every $\alpha \geq 0$ and $n \in \naturals$. From the central
limit theorem (CLT), since the RVs
$\{Y_i\}_{i=0}^{\infty}$ are i.i.d. with zero mean and
variance~$d^2$, then
$\frac{1}{\sqrt{n}} (X_n - X_0) = \frac{1}{\sqrt{n}} \sum_{k=1}^n Y_k$
converges in distribution to
$\mathcal{N}(0,d^2)$. Therefore, for every $\alpha \geq 0$,
\begin{equation}
\lim_{n \rightarrow \infty} \pr( | X_n - X_0 | \geq \alpha
\sqrt{n})= 2 \, Q\Bigl(\frac{\alpha}{d}\Bigr)
\label{CLT1 - i.i.d. RVs}
\end{equation}
where
\begin{equation}
Q(x) \triangleq \frac{1}{\sqrt{2\pi}} \, \int_{x}^{\infty}
\exp\Bigl(-\frac{t^2}{2}\Bigr) \mathrm{d}t, \quad \forall \, x \in
\reals \label{eq: Q function}
\end{equation}
is the probability that a zero-mean and unit-variance Gaussian
RV is larger than $x$. Since the following exponential
upper and lower bounds on the Q-function hold
\begin{equation}
\frac{1}{\sqrt{2\pi}} \, \frac{x}{1+x^2} \cdot e^{-\frac{x^2}{2}}
< Q(x) < \frac{1}{\sqrt{2\pi} \, x} \cdot e^{-\frac{x^2}{2}}, \;
\; \forall \, x>0 \label{eq: upper and lower bounds for the Q
function}
\end{equation}
then it follows from \eqref{CLT1 - i.i.d. RVs} that the exponent
on the right-hand side of \eqref{eq: Azuma's inequality for
example1} is the exact exponent in this example. \label{example1}
\end{example}

\begin{example}
In continuation to Example~\ref{example1}, let $\gamma \in (0,1]$,
and let us generalize this example by considering the case where the
i.i.d. binary RVs $\{Y_i\}_{i=0}^{\infty}$ have the
probability law
$$ \pr(Y_i = +d) = \frac{\gamma}{1+\gamma}, \quad \pr(Y_i =
-\gamma d) = \frac{1}{1+\gamma} \; .$$ Hence, it follows that the
i.i.d. RVs $\{Y_i\}$ have zero mean and variance $\sigma^2 =
\gamma d^2$. Let $\{X_k,
\mathcal{F}_k\}_{k=0}^{\infty}$ be defined similarly to
Example~\ref{example1}, so that it forms a martingale sequence.
Based on the CLT, $\frac{1}{\sqrt{n}} \, (X_n - X_0) =
\frac{1}{\sqrt{n}} \, \sum_{k=1}^n Y_k$ converges weakly to
$\mathcal{N}(0, \gamma d^2)$, so for every $\alpha \geq 0$
\begin{equation}
\lim_{n \rightarrow \infty} \pr( | X_n - X_0 | \geq \alpha
\sqrt{n})= 2 \, Q\biggl(\frac{\alpha}{\sqrt{\gamma} \, d}\biggr).
\label{CLT2 - i.i.d. RVs}
\end{equation}
From the exponential upper and lower bounds of the Q-function in
\eqref{eq: upper and lower bounds for the Q function}, the
right-hand side of \eqref{CLT2 - i.i.d. RVs} scales exponentially
like $e^{-\frac{\alpha^2}{2 \gamma d^2}}$. Hence, the exponent in
this example is improved by a factor $\frac{1}{\gamma}$ as
compared Azuma's inequality (that is the same as in
Example~\ref{example1} since $|X_k - X_{k-1}| \leq d$ for every $k
\in \naturals$). This indicates on the possible refinement of
Azuma's inequality by introducing an additional constraint on the
second moment. This route was studied extensively in the
probability literature, and it is the focus of
Section~\ref{section: Refined Versions of Azuma's Inequality}.
\label{example2}
\end{example}

\subsection{McDiarmid's inequality}
\label{subsection: McDiarmid's inequality}
The following useful inequality is due to McDiarmid
(\cite[Theorem~3.1]{McDiarmid_1997} or
\cite{McDiarmid_bounded_differences_Martingales_1989}),
and its original derivation uses the martingale approach for its
derivation. We will relate, in the following, the derivation of this
inequality to the derivation of the Azuma-Hoeffding inequality
(see the preceding subsection).

\begin{theorem}{\bf[McDiarmid's inequality]}
Let $\{X_k\}_{k=1}^n$ be independent real-valued random variables,
taking values in the set
$\mathcal{X} \deq \prod_{k=1}^n \mathcal{X}_k \subseteq \reals^n$.
Let $g: \mathcal{X} \rightarrow \reals$ be a measurable function
such that, for some constants $\{d_k\}_{k=1}^n$,
\begin{equation}
\bigl| g(\underline{x}) - g(\underline{x}') \bigr| \leq d_k,
\quad \forall \, k \in \{1, \ldots, n\}
\label{eq: assumption on the variation of g}
\end{equation}
where $\underline{x} = (x_1, \ldots, x_{k-1}, x_k, x_{k+1}, \ldots, x_n)$
and $\underline{x}' = (x_1, \ldots, x_{k-1}, x'_k, x_{k+1}, \ldots, x_n)$
are two arbitrary points in the set $\mathcal{X}$ that may only differ in their
$k$-th coordinate (this is equivalent to saying that the variation of the
function $g$ w.r.t. its $k$-th coordinate is upper bounded by $d_k$).
Then, for every $\alpha \geq 0$,
\begin{equation}
\pr( \bigl| g(X_1, \ldots, X_n) - \expectation \bigl[g(X_1, \ldots, X_n) \bigr] \bigr|
\geq \alpha) \leq 2 \exp \left(-\frac{2 \alpha^2}{\sum_{k=1}^n d_k^2} \right).
\label{eq: McDiarmid's inequality}
\end{equation}
\label{theorem: McDiarmid's inequality}
\end{theorem}

\begin{remark}
One can use the Azuma-Hoeffding inequality for a derivation of a
concentration inequality in the considered setting. However, the following
proof provides in this setting an improvement by a factor of~4 in the exponent
of the bound.
\end{remark}

\begin{proof}
For $k \in \{1, \ldots, n\}$, let
$\mathcal{F}_k = \sigma(X_1, \ldots, X_k)$ be the $\sigma$-algebra that is generated
by $X_1, \ldots, X_k$ with $\mathcal{F}_0 = \{\emptyset, \Omega\}$. Define
\begin{equation}
\xi_k \triangleq \expectation \bigl[ g(X_1, \ldots, X_n) \, | \, \mathcal{F}_k \bigr]
- \expectation \bigl[ g(X_1, \ldots, X_n) \, | \, \mathcal{F}_{k-1} \bigr], \quad
\forall \, k \in \{1, \ldots, n\}.
\label{eq: martingale difference}
\end{equation}
Note that $\mathcal{F}_0 \subseteq \mathcal{F}_1 \ldots \subseteq \mathcal{F}_n$
is a filtration, and
\begin{eqnarray}
&& \expectation \bigl[ g(X_1, \ldots, X_n) \, | \, \mathcal{F}_0 \bigr] =
\expectation \bigl[ g(X_1, \ldots, X_n) \bigr] \nonumber \\
&& \expectation \bigl[ g(X_1, \ldots, X_n) \, | \, \mathcal{F}_n \bigr] =
g(X_1, \ldots, X_n).
\end{eqnarray}
Hence, it follows from the last three equalities that
\begin{equation*}
g(X_1, \ldots, X_n) - \expectation \bigl[ g(X_1, \ldots, X_n) \bigr] = \sum_{k=1}^n \xi_k.
\end{equation*}
In the following, we need a lemma:
\begin{lemma}
For every $k \in \{1, \ldots, n\}$, the following properties hold a.s.:
\begin{enumerate}
\item $\expectation[\xi_k \, | \, \mathcal{F}_{k-1}] = 0$, so $\{\xi_k, \mathcal{F}_k\}$
is a martingale-difference and $\xi_k$ is $\mathcal{F}_k$-measurable.
\item $|\xi_k| \leq d_k$
\item $\xi_k \in [A_k, A_k + d_k]$ where $A_k$ is some non-positive and
$\mathcal{F}_{k-1}$-measurable random variable.
\end{enumerate}
\end{lemma}
\begin{proof}
The random variable $\xi_k$ is $\mathcal{F}_k$-measurable since
$\mathcal{F}_{k-1} \subseteq \mathcal{F}_k$, and $\xi_k$ is a difference
of two functions where one is $\mathcal{F}_k$-measurable and the other
is $\mathcal{F}_{k-1}$-measurable. Furthermore, it is easy to verify that
$\expectation[\xi_k \, | \, \mathcal{F}_{k-1}] = 0.$ This proves the first item.
The second item follows from the first and third items. To prove the third item,
note that $\xi_k = f_k(X_1, \ldots, X_k)$ holds a.s. for some
function $f_k: \mathcal{X}_1 \times \ldots \times \mathcal{X}_k \rightarrow \reals$
that is $\mathcal{F}_k$-measurable. Let us define, for every $k \in \{1, \ldots, n\}$,
\begin{eqnarray*}
&& A_k \triangleq \inf_{x \in \mathcal{X}_k} f_k(X_1, \ldots, X_{k-1}, x), \\
&& B_k \triangleq \sup_{x \in \mathcal{X}_k} f_k(X_1, \ldots, X_{k-1}, x)
\end{eqnarray*}
which are $\mathcal{F}_{k-1}$-measurable, and by definition
$\xi_k \in [A_k, B_k]$ holds almost surely. Furthermore, for every point
$(x_1, \ldots, x_{k-1}) \in \mathcal{X}_1 \times \ldots \times \mathcal{X}_{k-1}$,
we have
\begin{eqnarray}
&& \sup_{x \in \mathcal{X}_k} f_k(x_1, \ldots, x_{k-1}, x) -
\inf_{x' \in \mathcal{X}_k} f_k(x_1, \ldots, x_{k-1}, x') \nonumber \\
&& = \sup_{x, x' \in \mathcal{X}_k} \bigl\{ f_k(x_1, \ldots, x_{k-1}, x) -
f_k(x_1, \ldots, x_{k-1}, x') \bigr\} \nonumber \\
&& = \sup_{x, x' \in \mathcal{X}_k} \Bigl\{
\expectation \bigl[ g(X_1, \ldots, X_n) \, | \, X_1 = x_1, \ldots, X_{k-1} = x_{k-1}, X_k=x] \nonumber \\
&& \hspace*{1.7cm} - \expectation \bigl[ g(X_1, \ldots, X_n) \, | \, X_1 = x_1, \ldots, X_{k-1} = x_{k-1}, X_k=x'] \Bigr\} \nonumber \\
&& = \sup_{x, x' \in \mathcal{X}_k} \Bigl\{
\expectation \bigl[ g(x_1, \ldots, x_{k-1}, x, X_{k+1}, \ldots, X_n)]
- \expectation \bigl[ g(x_1, \ldots, x_{k-1}, x', X_{k+1}, \ldots, X_n)] \Bigr\}
\label{eq: independence} \\
&& = \sup_{x, x' \in \mathcal{X}_k} \Bigl\{
\expectation \bigl[ g(x_1, \ldots, x_{k-1}, x, X_{k+1}, \ldots, X_n)
- g(x_1, \ldots, x_{k-1}, x', X_{k+1}, \ldots, X_n)] \Bigr\} \nonumber \\
&& \leq d_k \label{eq: bound on the variation of g w.r.t. k-th coordinate}
\end{eqnarray}
where \eqref{eq: independence} follows from the independence of the random variables
$\{X_k\}_{k=1}^n$, and \eqref{eq: bound on the variation of g w.r.t. k-th coordinate}
follows from the condition in \eqref{eq: assumption on the variation of g}. Hence, it
follows that $B_k - A_k \leq d_k$ a.s., which then implies that $\xi_k \in [A_k, A_k+d_k]$.
Since $\expectation[\xi_k \, | \, \mathcal{F}_{k-1}] = 0$ then
a.s. the $\mathcal{F}_{k-1}$-measurable function $A_k$ is non-positive. It is
noted that the third item of the lemma makes it different from the
proof of the Azuma-Hoeffding inequality (in that case, it implies that
$\xi_k \in [-d_k, d_k]$ where the length of the interval is twice larger.)
\end{proof}
%Since $\expectation[\xi_k \, | \, \mathcal{F}_{k-1}] = 0$
%and $\xi_k \in [A_k, A_k + d_k]$ with $A_k \leq 0$ and $\mathcal{F}_{k-1}$-measurable,
%then $$\text{Var}(\xi_k \, | \, \mathcal{F}_{k-1}) \leq -A_k (A_k+d_k) \triangleq \sigma_k^2.$$
Applying the convexity of the exponential function (similarly to the derivation
of the Azuma-Hoeffding inequality, but this time w.r.t. the interval $[A_k, A_k+d_k]$)
implies that for every $k \in \{1, \ldots, n\}$
\begin{eqnarray*}
&& \expectation[e^{t \xi_k} \, | \, \mathcal{F}_{k-1}] \\
&& \leq \expectation\left[ \frac{(\xi_k - A_k) e^{t(A_k+d_k)} + (A_k+d_k-\xi_k) e^{t A_k}}{d_k} \,
\Big| \, \mathcal{F}_{k-1}\right] \\
&& = \frac{(A_k+d_k) e^{t A_k} - A_k e^{t (A_k+d_k)}}{d_k}.
\end{eqnarray*}
Let $P_k \triangleq -\frac{A_k}{d_k} \in [0,1]$, then
\begin{eqnarray}
&& \expectation[e^{t \xi_k} \, | \, \mathcal{F}_{k-1}] \nonumber \\
&& \leq P_k e^{t (A_k+d_k)} + (1-P_k) e^{t A_k} \nonumber \\
&& = e^{t A_k} \bigl( 1-P_k + P_k e^{t d_k} \bigr) \nonumber \\
&& = e^{H_k(t)}
\label{eq: upper bound on the conditional expectation for McDiadmid's inequality}
\end{eqnarray}
where
\begin{equation}
H_k(t) \triangleq t A_k + \ln \bigl(1-P_k + P_k e^{t d_k}\bigr), \quad \, \forall \, t \in \reals.
\label{eq: H_k}
\end{equation}
Since $H_k(0) = H_k'(0) = 0$ and the geometric mean is less than or equal to the arithmetic
mean then, for every $t$,
$$H_k''(t) = \frac{d_k^2 P_k (1-P_k) e^{t d_k}}{(1-P_k+P_k e^{t d_k})^2} \leq \frac{d_k^2}{4}$$
which implies by Taylor's theorem that
\begin{equation}
H_k(t) \leq \frac{t^2 d_k^2}{8}
\label{eq: upper bound on H_k used for Hoeffding inequality}
\end{equation}
so, from \eqref{eq: upper bound on the conditional expectation for McDiadmid's inequality},
\begin{equation*}
\expectation[e^{t \xi_k} \, | \, \mathcal{F}_{k-1}] \leq e^{\frac{t^2 d_k^2}{8}} \, .
\end{equation*}
Similarly to the proof of the Azuma-Hoeffding inequality, by repeatedly using
the recursion in \eqref{eq: smoothing theorem}, the last inequality implies that
\begin{equation}
\expectation \biggl[ \exp \biggl(t \sum_{k=1}^n \xi_k \biggr)
\biggr] \leq \exp \left(\frac{t^2}{8} \, \sum_{k=1}^n d_k^2 \right)
\end{equation}
which then gives from \eqref{eq: Chernoff's inequality} that, for every $t \geq 0$,
\begin{eqnarray}
&& \pr(g(X_1, \ldots, X_n) - \expectation[g(X_1, \ldots, X_n)] \geq \alpha) \nonumber \\
&& = \pr\left( \sum_{k=1}^n \xi_k \geq \alpha \right) \nonumber \\
&& \leq \exp\left(-\alpha t + \frac{t^2}{8} \, \sum_{k=1}^n d_k^2 \right).
\end{eqnarray}
An optimization over the free parameter $t \geq 0$ gives that
$t = 4\alpha \left(\sum_{k=1}^n d_k^2\right)^{-1}$, so
\begin{equation}
\pr(g(X_1, \ldots, X_n) - \expectation[g(X_1, \ldots, X_n)] \geq \alpha)
\leq \exp \left(-\frac{2 \alpha^2}{\sum_{k=1}^n d_k^2} \right).
\label{eq: one-sided McDiarmid's inequality}
\end{equation}
By replacing $g$ with $-g$, it follows that this bound is also valid for the probability
$$\pr\bigl(g(X_1, \ldots, X_n) - \expectation[g(X_1, \ldots, X_n)] \leq -\alpha \bigr)$$
which therefore gives the bound in \eqref{eq: McDiarmid's inequality}. This completes the
proof of Theorem~\ref{theorem: McDiarmid's inequality}.
\end{proof}

\subsection{Hoeffding's inequality, and its improved version (the Kearns-Saul inequality)}

In the following, we derive a concentration inequality for sums of independent and bounded
random variables as a consequence of McDiarmid's inequality. This inequality is due to
Hoeffding (see \cite[Theorem~2]{Hoeffding}). An improved version of Hoeffding's inequality,
due to Kearns and Saul \cite{KS_1998}, is also introduced in the following.

\bigskip
\begin{theorem}[Hoeffding]
Let $\{U_k\}_{k=1}^n$ be a sequence of independent and bounded random variables such that,
for every $k \in \{1, \ldots, n\}$, $U_k \in [a_k, b_k]$ holds a.s. for some constants
$a_k, b_k \in \reals$. Let $\mu_n \triangleq \sum_{k=1}^n \expectation[U_k]$.
Then,
\begin{equation}
\pr\left( \left| \sum_{k=1}^n U_k - \mu_n \right| \geq \alpha \sqrt{n} \right)
\leq 2 \exp\left(-\frac{2 \alpha^2 \, n}{\sum_{k=1}^n (b_k-a_k)^2} \right), \quad \forall \, \alpha \geq 0.
\label{eq: Hoeffding inequality}
\end{equation}
\label{theorem: Hoeffding inequality}
\end{theorem}
\begin{proof}
Apply Theorem~\ref{theorem: McDiarmid's inequality} to
$g(\underline{u}) \triangleq \sum_{k=1}^n u_k$ for every $\underline{u} \in \prod_{k=1}^n [a_k, b_k]$.
\end{proof}
\section{The main ingredients of the entropy method}
\label{sec:ingredients}

As a reminder, we are interested in the following question. Let $X_1,\ldots,X_n$ be $n$ independent random variables, each taking values in a set $\cX$. Given a function $f : \cX^n \to \reals$, we would like to find tight upper bounds on the {\em deviation probabilities} for the random variable $U = f(X^n)$, i.e., we wish to bound from above the probability $\pr (|U - \expectation U| \ge r)$ for each $r > 0$. Of course, if $U$ has finite variance, then Chebyshev's inequality already gives
\begin{align}\label{eq:Chebyshev_deviation_bound}
	\pr (|U - \expectation U| \ge r) \le \frac{\var(U)}{r^2}, \quad \forall \, r > 0.
\end{align}
However, in many instances a bound like \eqref{eq:Chebyshev_deviation_bound} is not nearly as tight as one would like, so ideally we aim for Gaussian-type bounds
\begin{align}\label{eq:Gaussian_deviation_bound}
	\pr (|U - \expectation U| \ge r) \le K \exp \left(-\kappa r^2 \right), \quad \forall \, r > 0
\end{align}
for some constants $K, \kappa > 0$. Whenever such a bound is available, $K$ is typically a small constant (usually, $K=2$), while $\kappa$ depends on the sensitivity of the function $f$ to variations in its arguments.

In the preceding chapter, we have demonstrated the martingale method for deriving Gaussian concentration bounds of the form \eqref{eq:Gaussian_deviation_bound}. In this chapter, our focus is on the so-called ``entropy method,'' an information-theoretic technique that has become increasingly popular starting with the work of Ledoux \cite{Ledoux_paper} (see also \cite{Ledoux}). In the following, we will always assume (unless specified otherwise) that the function $f : \cX^n \to \reals$ and the probability distribution $P$ of $X^n$ are such that
\begin{itemize}
	\item $U = f(X^n)$ has zero mean: $\expectation U = \expectation f(X^n) = 0$
	\item $U$ is {\em exponentially integrable}:
	\begin{align}\label{eq:U_expint}
		\expectation[\exp(\lambda U)] = \expectation\left[\exp\big(\lambda f(X^n)\big)\right] < \infty, \qquad \forall \lambda \in \reals
	\end{align}
	[another way of writing this is $\exp(\lambda f) \in L^1(P)$ for all $\lambda \in \reals$].
\end{itemize}
In a nutshell, the entropy method has three basic ingredients:
\begin{enumerate}
	\item {\bf The Chernoff bounding trick} --- using Markov's inequality, the problem of bounding the deviation probability $\pr (|U - \expectation U| \ge r)$ is reduced to the analysis of the {\em logarithmic moment-generating function} $\Lambda(\lambda) \deq \ln \expectation[\exp(\lambda U)]$, $\lambda \in \reals$.
	\item {\bf The Herbst argument} --- the function $\Lambda(\lambda)$ is related through a simple first-order differential equation to the relative entropy (information divergence) $D(P^{(\lambda f)} \| P)$, where $P = P_{X^n}$ is the probability distribution of $X^n$ and $P^{(\lambda f)}$ is the {\em tilted probability distribution} defined by
	\begin{align}\label{eq:tilted_distribution}
		\frac{\d P^{(\lambda f)}}{\d P} = \frac{\exp(\lambda f)}{\expectation[\exp(\lambda f)]} = \exp\big(\lambda f - \Lambda(\lambda)\big).
		\end{align}
If the function $f$ and the probability distribution $P$ are such that
\begin{align}\label{eq:divergence_quadratic_bound}
D(P^{(\lambda f)} \| P) \le \frac{c\lambda^2}{2}
\end{align}
for some $c > 0$, then the Gaussian bound \eqref{eq:Gaussian_deviation_bound} holds with $K = 2$ and $\kappa = \frac{1}{2c}$. The standard way to establish  \eqref{eq:divergence_quadratic_bound} is through the so-called {\em logarithmic Sobolev inequalities}.
\item {\bf Tensorization of the entropy} --- with few exceptions, it is rather difficult to derive a bound like \eqref{eq:divergence_quadratic_bound} directly. Instead, one typically takes a divide-and-conquer approach: Using the fact that $P_{X^n}$ is a product distribution (by the assumed independence of the $X_i$'s), the divergence $D(P^{(\lambda f)} \| P)$ is bounded from above by a sum of ``one-dimensional'' (or ``local'') conditional divergence terms
\begin{align}\label{eq:local_divergence_term}
D\Big(P^{(\lambda f)}_{X_i|\bar{X}^i} \big\| P_{X_i} \big| P^{(\lambda f)}_{\bar{X}^i} \Big),
\qquad i = 1,\ldots, n
\end{align}
where, for each $i$, $\bar{X}^i \in \cX^{n-1}$ denotes the $(n-1)$-tuple obtained from $X^n$ by removing the $i$th coordinate, i.e., $\bar{X}^i = (X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_n)$. Despite their formidable appearance, the conditional divergences in \eqref{eq:local_divergence_term} are easier to handle because, for each given realization $\bar{X}^i = \bar{x}^i$, the $i$th such term involves a single-variable function $f_i(\cdot|\bar{x}^i) : \cX \to \reals$ defined by $f_i(y|\bar{x}^i) \deq f(x_1,\ldots,x_{i-1},y,x_{i+1},\ldots,x_n)$ and the corresponding tilted distribution $P^{(\lambda f)}_{X_i|\bar{X}^i = \bar{x}^i}$, where
\begin{align}\label{eq:local_tilting}
	\frac{\d P^{(\lambda f)}_{X_i|\bar{X}^i = \bar{x}^i}}{\d P_{X_i}} = \frac{\exp\big(\lambda f_i(\cdot|\bar{x}^i)\big)}{\expectation \left[\exp\big(\lambda f_i(X_i|\bar{x}^i)\big)\right]}, \qquad \forall \bar{x}^i \in \cX^{n-1}.
\end{align}
In fact, from \eqref{eq:tilted_distribution} and \eqref{eq:local_tilting}, it is easy to see that the conditional distribution $P^{(\lambda f)}_{X_i|\bar{X}^i = \bar{x}^i}$ is nothing but the tilted distribution $P^{(\lambda f_i(\cdot|\bar{x}^i))}_{X_i}$. This simple observation translates into the following: If the function $f$ and the probability distribution $P = P_{X^n}$ are such that there exist constants $c_1,\ldots,c_n > 0$ so that
\begin{align}\label{eq:local_divergence_quadratic_bound}
	D\Big(P^{(\lambda f_i(\cdot|\bar{x}^i))}_{X_i} \Big\| P_{X_i} \Big) \le \frac{c_i \lambda^2}{2}, \qquad \forall i \in \{1,\ldots,n\}, \bar{x}^i \in \cX^{n-1},
\end{align}
then \eqref{eq:divergence_quadratic_bound} holds with $c = \sum^n_{i=1}c_i$ (to be shown explicitly later),
which in turn gives that
\begin{align}
	\pr \Big( \left|f(X^n) - \expectation f(X^n) \right| \ge r \Big) \le 2 \exp\left( - \frac{r^2}{2 \sum^n_{i=1}c_i} \right), \quad r > 0.
\end{align}
Again, one would typically use logarithmic Sobolev inequalities to verify \eqref{eq:local_divergence_quadratic_bound}.
\end{enumerate}
In the remainder of this section, we shall elaborate on these three ingredients. Logarithimic Sobolev inequalities and their applications to concentration bounds are described in detail in Sections~\ref{sec:Gaussian_LSI} and \ref{sec:LSI}.

\subsection{The Chernoff bounding trick}

The first ingredient of the entropy method is the well-known Chernoff bounding trick\footnote{The name of H.~Chernoff is associated with this technique because of his 1952 paper \cite{Chernoff}; however, its roots go back to S.N.~Bernstein's 1927 textbook on the theory of probability \cite{Bernstein}.}: Using Markov's inequality, for any $\lambda > 0$ we have
\begin{align*}
	\pr (U  \ge r) &= \pr \big(\exp(\lambda U) \ge \exp(\lambda r)\big)\\
	&\le \exp(-\lambda r) \expectation[\exp(\lambda U)].
\end{align*}
Equivalently, if we define the {\em logarithmic moment generating function} $\Lambda(\lambda) \deq \ln \expectation[\exp(\lambda U)]$, $\lambda \in \reals$, we can write
\begin{align}\label{eq:upper_tail}
	\pr (U \ge r) & \le \exp\big(\Lambda(\lambda)-\lambda r\big), \qquad \forall \lambda > 0.
\end{align}
To bound the probability of the lower tail, $\pr (U \le - r)$, we follow the same steps, but with $-U$ instead of $U$. From now on, we will focus on the deviation probability $\pr (U \ge r)$.

By means of the Chernoff bounding trick, we have reduced the problem of bounding the deviation probability $\pr (U \ge r)$ to the analysis of the logarithmic moment-generating function $\Lambda(\lambda)$. The following properties of $\Lambda(\lambda)$ will be useful later on:
\begin{itemize}
	\item $\Lambda(0) = 0$
	\item Because of the exponential integrability of $U$ [cf.~\eqref{eq:U_expint}], $\Lambda(\lambda)$ is infinitely differentiable, and  one can interchange derivative and expectation. In particular,
	\begin{align}\label{eq:LMGF_derivatives}
		\Lambda'(\lambda) = \frac{\expectation[U \exp(\lambda U)]}{\expectation[\exp(\lambda U)]} \qquad \text{and} \qquad
		\Lambda''(\lambda) = \frac{\expectation[U^2 \exp(\lambda U)]}{\expectation[\exp(\lambda U)]} - \left( \frac{\expectation[U \exp(\lambda U)]}{\expectation[\exp(\lambda U)]} \right)^2
	\end{align}
	Since we have assumed that $\expectation U = 0$, we have $\Lambda'(0) = 0$ and $\Lambda''(0) = \var (U)$.
	\item Since $\Lambda(0) = \Lambda'(0) = 0$, we get
	\begin{align}\label{eq:zero_lambda_limit}
		\lim_{\lambda \to 0} \frac{\Lambda(\lambda)}{\lambda} = 0.
	\end{align}
\end{itemize}


\section{Applications}
The word \textit{applications} means different things to different people. If we wear our Mathematican hats we are always looking 
for a new tool to prove some theorem. Or reprove a celebrated theorem in a new way. Computer Scientists are always concerned with algorithms and performanace analysis and statisticians or \textit{data scientists} are after getting a handle on the asymptotics of estimators and samplers. In short, each specialist wants to know what a given tool will contribute to his field. 
\paragraph{} The remarkable thing about concentration of measure is that is uses span a wide range - from something practical as decoding neural signals to esoteric topics such as analyzing convex bodies in Banach spaces. 
It is too much to go beyond one or two applications.
So I'll cite some here 
\begin{itemize}
\item  Milman's proof of Dvoretzky's theorem on sections of convex bodies
\item a widely cited lemma of Johnson and Lindenstrauss concerning low-distortion dimensionality reduction in $\mathbb{R}^{n}$ by random projections.
\item statistics and empirical processes and machine learning\cite{BBL04b}
\end{itemize}
In the interest of keeping this document short we shall only look at the last example.
\subsection{Rademacher Processes}
A key technique in the theory of empirical processes is \textit{Rademacher symmetrization}. This was first introduced into empirical processes in a classical paper by Gine \cite{Gine1984}
so we'll show how this applies in the context of Talagrand's inequality. 
\paragraph{} Let $\epsilon_i,i=1,\cdots,n,$ be i.i.d Rademacher random signs (taking values -1,1 with probability 1/2), independent of the $X'_{i}s$, defined on a large product probability space with product probability Pr, denote the joint expectation by E, and the $E_{\epsilon}$ and $E_{X}$ the corresponding expectations w.r.t the $\epsilon_i 's$ $X'_{i}s$,
respectively. The following symmetrization inequality holds for random variables in arbitrary normed spaces, but we state it for the suprenum norm relevant in empirical process theory: For $\mathcal{F}$ a class of functions on $(S,\mathcal{A})$, define $\|
H\|_{\mathcal{F}} = \mathrm{sup}_{f \in \mathcal{F}} |H(f)|$.

\begin{lemma}
Let $\mathcal{F}$ be a uniformly bounded P-centered class of functions defined on a measurable space $(S,\mathcal{A})$. Let 
$\epsilon_{i}$ be i.i.d. Rademachers as above, and let $a_i, i = 1,\cdots,n$ be any sequence of real numbers. Then
\begin{equation}
\dfrac{1}{2}E \|\sum_{i=1}^{n} \epsilon_{i} f(X_{i})\|_{\mathcal{F}} \leq 
E \|\sum_{i=1}^{n} f(X_i)\|_{\mathcal{F}} \leq \|\sum_{i=1}^{n} \epsilon_{i}(f(X_i)+a_{i})\|_{\mathcal{F}}
\end{equation}
\end{lemma}
\begin{proof}
Let us assume for simplictly that $\mathcal{F}$ is countable (so
that we can neglect measurability problems). Since $E_{X}f(X_{i}) = 0$ for every $f,i,$ the first inequality follows from
\begin{equation*}
E \|\sum_{i=1}^{n} \epsilon_{i}(f(X_i)\|_{\mathcal{F}}
= E_{\epsilon} E_{X} \leq E_{\epsilon} E_{X} \|\sum_{i: \epsilon_{i} = -1}f(X_{i}) + E_{X} \sum_{i: \epsilon_{i} =1}f(X_{i})\|_{\mathcal{F}}
 + E_{\epsilon} E_{X} \|\sum_{i: \epsilon_{i} = 1}f(X_{i}) + E_{X} \sum_{i: \epsilon_{i} = -1}f(X_{i})\|_{\mathcal{F}}
 \leq 2E \|\sum_{i=1}^{n} f(X_{i})\|_{\mathcal{F}}
\end{equation*}
where in the last inequality we have used Jensen's inequality and convexity of the norm. To prove the second inequality, let 
$X_{n+i}, i =1,\cdots,n$ be an independent copy of $X_1, \cdots, 
X_n$. Then proceeding as above,
$E\|\sum_{i=1}^{n} f(X_{i})\|_{\mathcal{F}} = E \|\sum_{i=1}^{n}(f(X_{i}) - E f(X_{n+i})\|_{\mathcal{F}} \leq E \|\sum_{i=1}^{n}(f(X_{i} + a_i) - \sum_{i=1}^{n}(f(X_{n+i}+a_i)\|_{\mathcal{F}}$

which clearly equals
\begin{equation*}
E_{\epsilon}E_X \|\sum_{i: \epsilon_{i}=1} \epsilon_i (f(X_i) + a_i - f(X_{n+i}) -a_{i}
- \sum_{i: \epsilon_{i}=-1} \epsilon_i (f(X_i) + a_i - f(X_{n+i}) -a_{i}\|_{\mathcal{F}}
\end{equation*}
Now Pr being a product probability measure with identical coordinates, it is invariant by permutations of the coordinates, so that we may exchange $f(X_i)$ and $f(X_{n+i})$ for the i's where
$\epsilon_i = -1$ in the last expectation. This gives that the quantity in the last equation equals 
\begin{equation*}
 E_{\epsilon} E_X \|\sum_{i=1}^{n} \epsilon_i(f(X_i)) + a_{i} - f(X_{n+i}) - a_{i})\|_{\mathcal{F}}\leq 2E \|\sum_{i=1}^{n} \epsilon_i(f(X_{i}) +a_i)\|_{\mathcal{F}}
\end{equation*} which completes the proof.
\end{proof}
This simple but very useful result says that we can always compare the size of the expectation of the supremum of an empirical process to a symmetrized process. The idea usual is that the symmetrized \textit{Rademacher} process has conditional on the $X_i's$ a very simple structure. One can then derive results of the Rademacher proecess and integrate the results over the distribution of the  $X_i's$

\section{Rademacher Chaos}\label{Rademacher_averages}
Rademacher chaos and Rademacher averages are quantities
that play an important role in empirical process
theory [TK:Define - Empirical process theory] and in
the theory of Banach spaces [TK: Include references here to Ledoux and Talagrand and Wellner]
\subsection{Rademacher averages}
Let B denote a seperable Banach space and let $X_1, 
\cdots , X_n$ be independent and identically distributed bounded B-valued random variables. Without loss of generality we assume that $\|X_1\| \leq 1$ almost surely. The quantity of interest is the conditional Rademacher average$ Z 
= \expectation\[\|\sum_{i=1}^{n} \epsilon_i X_i \| | X^n_1\right]$ where the $\epsilon_i$ are independent centered ${1, -1}$-valued random variables. We offer the following concentration inequalities for Z:
\begin{theorem}
For any $t \gt 0$, 
\begin{equation}
\mathbb{P}[Z \geq \expectation Z + t] \leq
\exp [- \dfrac{t^2}{2 \expectation Z + 2t/3}]
\end{equation}
and \begin{equation}
\mathbb{P}[Z \leq \expectation Z - t] \leq [- \dfrac{t^2}{2 \expectation Z }]
\end{equation}
We are particularly interested in deriving (upper and lower) tail inequalities for conditional Rademacher averages. This is significant in statistical applications. 
\end{theorem}
\subsection{Rademacher Chaos}
In this section $\mathcal{F}$ denotes a collection of n x n symmetric matrices M, 
and $\epsilon_1, \cdots, \epsilon_n$ are i.i.d. Rademacher variables. We assume 
that if $M \in \mathcal{F}$, then $-M \in \mathcal{F}$. To avoid problems with measurability we assume that $\mathcal{F}$ is a finite set. For convenience assume that the matrics M have zero 
diagonal, that is, M(i,i) = 0 for all M $\in \mathcal{F}$ and $i = 1, \cdots, n$. We investigate 
concentration of the random variable
\begin{equation*}
Z = \mathrm{sup}_{M \in \mathcal{F}} \sum_{i,j \leq n} \epsilon_i \epsilon_j M(i,j)
\end{equation*}
Suppose the supremum of the $L_2$ operator norm of matrices $(M)_{M \in \mathcal{F}}$ is finite,
and w.l.o.g we assume that this supremum equals one, that is,
\begin{equation*}
\mathrm{sup}_{M \in \mathcal{F}}  \mathrm{sup}_{\alpha: \sum_{i=1}^{n} \alpha^{2}_{i} \leq 1} 
\alpha^{\dagger} M \alpha = 1
\end{equation*}
where $\alpha^{\dagger}$ denotes the transpose of the vector $\alpha = (\alpha_1, \cdots, \alpha_n)
\in \mathbb{R}^n$.
We introduce an important theorem which follows from the previous result. 
\begin{thebibliography}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Talagrand96}
M.~Talagrand, ``A new look at independence,'' \emph{Annals of Probability},
  vol.~24, no.~1, pp. 1--34, January 1996.

\bibitem{Boucheron_Lugosi_Massart_book}
S.~Boucheron, G.~Lugosi, and P.~Massart, \emph{Concentration Inequalities - A
  Nonasymptotic Theory of Independence}.\hskip 1em plus 0.5em minus 0.4em\relax
  Oxford University Press, 2013.

\bibitem{Ledoux}
M.~Ledoux, \emph{The Concentration of Measure Phenomenon}, ser. Mathematical
  Surveys and Monographs.\hskip 1em plus 0.5em minus 0.4em\relax American
  Mathematical Society, 2001, vol.~89.

\bibitem{Lugosi_Lecture_Notes}
\BIBentryALTinterwordspacing
G.~Lugosi, ``Concentration of measure inequalities - lecture notes,'' 2009.
  [Online]. Available: \url{http://www.econ.upf.edu/~lugosi/anu.pdf.}
\BIBentrySTDinterwordspacing

\bibitem{Massart_book}
P.~Massart, \emph{The Concentration of Measure Phenomenon}, ser. Lecture Notes
  in Mathematics.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2007, vol.
  1896.

\bibitem{McDiarmid_tutorial}
C.~McDiarmid, ``Concentration,'' in \emph{Probabilistic Methods for Algorithmic
  Discrete Mathematics}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 1998,
  pp. 195--248.

\bibitem{Talagrand95}
M.~Talagrand, ``Concentration of measure and isoperimteric inequalities in
  product space,'' \emph{Publications Math{\'{e}}matiques de {l'I.H.E.S}},
  vol.~81, pp. 73--205, 1995.

\bibitem{Azuma}
K.~Azuma, ``Weighted sums of certain dependent random variables,'' \emph{Tohoku
  Mathematical Journal}, vol.~19, pp. 357--367, 1967.

\bibitem{Hoeffding}
W.~Hoeffding, ``Probability inequalities for sums of bounded random
  variables,'' \emph{Journal of the American Statistical Association}, vol.~58,
  no. 301, pp. 13--30, March 1963.

\bibitem{AlonS_tpm3}
N.~Alon and J.~H. Spencer, \emph{The Probabilistic Method}, 3rd~ed.\hskip 1em
  plus 0.5em minus 0.4em\relax Wiley Series in Discrete Mathematics and
  Optimization, 2008.

\bibitem{Chung_LU2006}
F.~Chung and L.~Lu, \emph{Complex Graphs and Networks}, ser. Regional
  Conference Series in Mathematics.\hskip 1em plus 0.5em minus 0.4em\relax
  Wiley, 2006, vol. 107.

\bibitem{survey2006}
\BIBentryALTinterwordspacing
------, ``Concentration inequalities and martingale inequalities: a survey,''
  \emph{Internet Mathematics}, vol.~3, no.~1, pp. 79--127, March 2006.
  [Online]. Available: \url{http://www.ucsd.edu/~fan/wp/concen.pdf.}
\BIBentrySTDinterwordspacing

\bibitem{RiU_book}
T.~J. Richardson and R.~Urbanke, \emph{Modern Coding Theory}.\hskip 1em plus
  0.5em minus 0.4em\relax Cambridge University Press, 2008.

\bibitem{SeldinLCTA_IT2012}
Y.~Seldin, F.~Laviolette, N.~Cesa-Bianchi, J.~Shawe-Taylor, and P.~Auer,
  ``{PAC}-{B}ayesian inequalities for martingales,'' \emph{IEEE Trans. on
  Information Theory}, vol.~58, no.~12, pp. 7086--7093, December 2012.

\bibitem{Tropp_FoCM_2011}
J.~A. Tropp, ``User-friendly tail bounds for sums of random matrices,''
  \emph{Foundations of Computational Mathematics}, vol.~12, no.~4, pp.
  389--434, August 2012.

\bibitem{Tropp_ECP_2011}
------, ``Freedman's inequality for matrix martingales,'' \emph{Electronic
  Communications in Probability}, vol.~16, pp. 262--270, March 2011.

\bibitem{Gozlan_Leonard}
N.~Gozlan and C.~Leonard, ``Transport inequalities: a survey,'' \emph{Markov
  Processes and Related Fields}, vol.~16, no.~4, pp. 635--736, 2010.

\bibitem{Steele_book}
J.~M. Steele, \emph{Probability Theory and Combinatorial Optimization}, ser.
  CBMS--NSF Regional Conference Series in Applied Mathematics.\hskip 1em plus
  0.5em minus 0.4em\relax Siam, Philadelphia, PA, USA, 1997, vol.~69.

\bibitem{Dembo}
A.~Dembo, ``Information inequalities and concentration of measure,''
  \emph{Annals of Probability}, vol.~25, no.~2, pp. 927--939, 1997.

\bibitem{Chatterjee_phd}
\BIBentryALTinterwordspacing
S.~Chatterjee, ``Concentration inequalities with exchangeable pairs,'' Ph.D.
  dissertation, Stanford University, California, USA, February 2008. [Online].
  Available: \url{http://arxiv.org/abs/0507526.}
\BIBentrySTDinterwordspacing

\bibitem{Chatterjee1}
------, ``Stein's method for concentration inequalities,'' \emph{Probability
  Theory and Related Fields}, vol. 138, pp. 305--321, 2007.

\bibitem{Chatterjee2}
S.~Chatterjee and P.~S. Dey, ``Applications of {S}tein's method for
  concentration inequalities,'' \emph{Annals of Probability}, vol.~38, no.~6,
  pp. 2443--2485, June 2010.

\bibitem{Ross2011}
N.~Ross, ``Fundamentals of {S}tein's method,'' \emph{Probability Surveys},
  vol.~8, pp. 210--293, 2011.

\bibitem{Abbe_Montanari_KSAT}
\BIBentryALTinterwordspacing
E.~Abbe and A.~Montanari, ``On the concentration of the number of solutions of
  random satisfiability formulas,'' 2010. [Online]. Available:
  \url{http://arxiv.org/abs/1006.3786.}
\BIBentrySTDinterwordspacing

\bibitem{Korada_Macris_ISIT07}
S.~B. Korada and N.~Macris, ``On the concentration of the capacity for a code
  division multiple access system,'' in \emph{Proceedings of the 2007 IEEE
  International Symposium on Information Theory}, Nice, France, June 2007, pp.
  2801--2805.

\bibitem{Korada_Kudekar_Macris_ISIT08}
S.~B. Korada, S.~Kudekar, and N.~Macris, ``Concentration of magnetization for
  linear block codes,'' in \emph{Proceedings of the 2008 IEEE International
  Symposium on Information Theory}, Toronto, Canada, July 2008, pp. 1433--1437.

\bibitem{Kudekar_thesis09}
\BIBentryALTinterwordspacing
S.~Kudekar, ``Statistical physics methods for sparse graph codes,'' Ph.D.
  dissertation, EPFL~-~Swiss Federal Institute of Technology, Lausanne,
  Switzeland, July 2009. [Online]. Available:
  \url{http://infoscience.epfl.ch/record/138478/files/EPFL_TH4442.pdf.}
\BIBentrySTDinterwordspacing

\bibitem{Kudekar_Macris_IT2009}
S.~Kudekar and N.~Macris, ``Sharp bounds for optimal decoding of low-density
  parity-check codes,'' \emph{IEEE Trans. on Information Theory}, vol.~55,
  no.~10, pp. 4635--4650, October 2009.

\bibitem{Korada_Macris_IT10}
S.~B. Korada and N.~Macris, ``Tight bounds on the capacity of binary input
  random {CDMA} systems,'' \emph{IEEE Trans. on Information Theory}, vol.~56,
  no.~11, pp. 5590--5613, November 2010.

\bibitem{Montanari05}
A.~Montanari, ``Tight bounds for {LDPC} and {LDGM} codes under {MAP}
  decoding,'' \emph{IEEE Trans. on Information Theory}, vol.~51, no.~9, pp.
  3247--3261, September 2005.

\bibitem{Talagrand_2010}
M.~Talagrand, \emph{Mean Field Models for Spin Glasses}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer-Verlag, 2010.

\bibitem{Bobkov_Madiman_paper2}
S.~Bobkov and M.~Madiman, ``Concentration of the information in data with
  log-concave distributions,'' \emph{Annals of Probability}, vol.~39, no.~4,
  pp. 1528--1543, 2011.

\bibitem{Bobkov_Madiman_paper1}
------, ``The entropy per coordinate of a random vector is highly constrained
  under convexity conditions,'' \emph{IEEE Trans. on Information Theory},
  vol.~57, no.~8, pp. 4940--4954, August 2011.

\bibitem{ShamirS87}
E.~Shamir and J.~Spencer, ``Sharp concentration of the chromatic number on
  random graphs,'' \emph{Combinatorica}, vol.~7, no.~1, pp. 121--129, 1987.

\bibitem{LubyMSS_IT01}
M.~G. Luby, Mitzenmacher, M.~A. Shokrollahi, and D.~A. Spielmann, ``Efficient
  erasure-correcting codes,'' \emph{IEEE Trans. on Information Theory},
  vol.~47, no.~2, pp. 569--584, February 2001.

\bibitem{RichardsonU2001}
T.~J. Richardson and R.~Urbanke, ``The capacity of low-density parity-check
  codes under message-passing decoding,'' \emph{IEEE Trans. on Information
  Theory}, vol.~47, no.~2, pp. 599--618, February 2001.

\bibitem{SipserS96}
M.~Sipser and D.~A. Spielman, ``Expander codes,'' \emph{IEEE Trans. on
  Information Theory}, vol.~42, no.~6, pp. 1710--1722, November 1996.

\bibitem{WagnerPK_IT11}
A.~B. Wagner, P.~Viswanath, and S.~R. Kulkarni, ``Probability estimation in the
  rare-events regime,'' \emph{IEEE Trans. on Information Theory}, vol.~57,
  no.~6, pp. 3207--3229, June 2011.

\bibitem{McDiarmid_1997}
C.~McDiarmid, ``Centering sequences with bounded differences,''
  \emph{Combinatorics, Probability and Computing}, vol.~6, no.~1, pp. 79--86,
  March 1997.

\bibitem{Volterra_ITW09}
K.~Xenoulis and N.~Kalouptsidis, ``On the random coding exponent of nonlinear
  {G}aussian channels,'' in \emph{Proceedings of the 2009 IEEE International
  Workshop on Information Theory}, Volos, Greece, June 2009, pp. 32--36.

\bibitem{Volterra_IT_March11}
------, ``Achievable rates for nonlinear {V}olterra channels,'' \emph{IEEE
  Trans. on Information Theory}, vol.~57, no.~3, pp. 1237--1248, March 2011.

\bibitem{ISIT2012_Voltera}
K.~Xenoulis, N.~Kalouptsidis, and I.~Sason, ``New achievable rates for
  nonlinear {V}olterra channels via martingale inequalities,'' in
  \emph{Proceedings of the 2012 IEEE International Workshop on Information
  Theory}, MIT, Boston, MA, USA, July 2012, pp. 1430--1434.

\bibitem{Ledoux_paper}
M.~Ledoux, ``On {T}alagrand's deviation inequalities for product measures,''
  \emph{{ESAIM:} Probability and Statistics}, vol.~1, pp. 63--87, 1997.

\bibitem{Gross}
L.~Gross, ``Logarithmic {S}obolev inequalities,'' \emph{American Journal of
  Mathematics}, vol.~97, no.~4, pp. 1061--1083, 1975.

\bibitem{Stam}
A.~J. Stam, ``Some inequalities satisfied by the quantities of information of
  {F}isher and {S}hannon,'' \emph{Information and Control}, vol.~2, pp.
  101--112, 1959.

\bibitem{Federbush}
P.~Federbush, ``A partially alternate derivation of a result of {N}elson,''
  \emph{Journal of Mathematical Physics}, vol.~10, no.~1, pp. 50--52, 1969.

\bibitem{Dembo_Cover_Thomas}
A.~Dembo, T.~M. Cover, and J.~A. Thomas, ``Information theoretic
  inequalities,'' \emph{IEEE Trans. on Information Theory}, vol.~37, no.~6, pp.
  1501--1518, November 1991.

\bibitem{Villani_EP_concavity}
C.~Villani, ``A short proof of the `concavity of entropy power','' \emph{IEEE
  Trans. on Information Theory}, vol.~46, no.~4, pp. 1695--1696, July 2000.

\bibitem{Toscani}
G.~Toscani, ``An information-theoretic proof of {N}ash's inequality,''
  \emph{Rendiconti Lincei: Matematica e Applicazioni}, 2012, in press.

\bibitem{Guionnet_Zegarlinski}
A.~Guionnet and B.~Zegarlinski, ``Lectures on logarithmic {S}obolev
  inequalities,'' \emph{S\'eminaire de probabilit\'es (Strasbourg)}, vol.~36,
  pp. 1--134, 2002.

\bibitem{Ledoux_lecture_notes}
M.~Ledoux, ``Concentration of measure and logarithmic {S}obolev inequalities,''
  in \emph{S\'eminaire de Probabilit\'es XXXIII}, ser. Lecture Notes in
  Math.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 1999, vol. 1709, pp.
  120--216.

\bibitem{Royer}
G.~Royer, \emph{An Invitation to Logarithmic Sobolev Inequalities}, ser.
  SFM/AMS Texts and Monographs.\hskip 1em plus 0.5em minus 0.4em\relax American
  Mathematical Society and Soci\'et\'e Math\'ematiques de France, 2007,
  vol.~14.

\bibitem{Bobkov_Gotze_expint}
S.~G. Bobkov and F.~G\"otze, ``Exponential integrability and transportation
  cost related to logarithmic {S}obolev inequalities,'' \emph{Journal of
  Functional Analysis}, vol. 163, pp. 1--28, 1999.

\bibitem{Bobkov_Ledoux}
S.~G. Bobkov and M.~Ledoux, ``On modified logarithmic {S}obolev inequalities
  for {B}ernoulli and {P}oisson measures,'' \emph{Journal of Functional
  Analysis}, vol. 156, no.~2, pp. 347--365, 1998.

\bibitem{Bobkov_Tetali}
S.~G. Bobkov and P.~Tetali, ``Modified logarithmic {S}obolev inequalities in
  discrete settings,'' \emph{Journal of Theoretical Probability}, vol.~19,
  no.~2, pp. 289--336, 2006.

\bibitem{Chafai}
D.~Chafa\"i, ``Entropies, convexity, and functional inequalities:
  ${\Phi}$-entropies and ${\Phi}$-{S}obolev inequalities,'' \emph{J. Math.
  Kyoto University}, vol.~44, no.~2, pp. 325--363, 2004.

\bibitem{KitsosT_IT09}
C.~P. Kitsos and N.~K. Tavoularis, ``Logarithmic {S}obolev inequalities for
  information measures,'' \emph{IEEE Trans. on Information Theory}, vol.~55,
  no.~6, pp. 2554--2561, June 2009.

\bibitem{Marton_dbar}
K.~Marton, ``Bounding $\bar{d}$-distance by informational divergence: a method
  to prove measure concentration,'' \emph{Annals of Probability}, vol.~24,
  no.~2, pp. 857--866, 1996.

\bibitem{Villani_TOT}
C.~Villani, \emph{Topics in Optimal Transportation}.\hskip 1em plus 0.5em minus
  0.4em\relax Providence, RI: American Mathematical Society, 2003.

\bibitem{Villani_newbook}
------, \emph{Optimal Transport: Old and New}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2008.

\bibitem{Cattiaux_Guillin}
P.~Cattiaux and A.~Guillin, ``On quadratic transportation cost inequalities,''
  \emph{Journal de Mat\'ematiques Pures et Appliqu\'ees}, vol.~86, pp.
  342--361, 2006.

\bibitem{Dembo_Zeitouni_TC}
A.~Dembo and O.~Zeitouni, ``Transportation approach to some concentration
  inequalities in product spaces,'' \emph{Electronic Communications in
  Probability}, vol.~1, pp. 83--90, 1996.

\bibitem{Djellout_Guillin_Wu}
H.~Djellout, A.~Guillin, and L.~Wu, ``Transportation cost-information
  inequalities and applications to random dynamical systems and diffusions,''
  \emph{Annals of Probability}, vol.~32, no.~3B, pp. 2702--2732, 2004.

\bibitem{Gozlan}
N.~Gozlan, ``A characterization of dimension free concentration in terms of
  transportation inequalities,'' \emph{Annals of Probability}, vol.~37, no.~6,
  pp. 2480--2498, 2009.

\bibitem{E_Milman}
E.~Milman, ``Properties of isoperimetric, functional and transport-entropy
  inequalities via concentration,'' \emph{Probability Theory and Related
  Fields}, vol. 152, pp. 475--507, 2012.

\bibitem{Gray_Neuhoff_Shields_dbar}
R.~M. Gray, D.~L. Neuhoff, and P.~C. Shields, ``A generalization of
  {O}rnstein's $\bar{d}$ distance with applications to information theory,''
  \emph{Annals of Probability}, vol.~3, no.~2, pp. 315--328, 1975.

\bibitem{Gray_Neuhoff_Omura}
R.~M. Gray, D.~L. Neuhoff, and J.~K. Omura, ``Process definitions of
  distortion-rate functions and source coding theorems,'' \emph{IEEE Trans. on
  Information Theory}, vol.~21, no.~5, pp. 524--532, September 1975.

\bibitem{Ahlswede_Gacs_Korner}
R.~Ahlswede, P.~G\'acs, and J.~K\"orner, ``Bounds on conditional probabilities
  with applications in multi-user communication,'' \emph{Z.
  Wahrscheinlichkeitstheorie verw. Gebiete}, vol.~34, pp. 157--177, 1976, see
  correction in vol.~39, no.~4, pp.~353--354, 1977.

\bibitem{Ahlswede_Dueck}
R.~Ahlswede and G.~Dueck, ``Every bad code has a good subcode: a local converse
  to the coding theorem,'' \emph{Z. Wahrscheinlichkeitstheorie verw. Gebiete},
  vol.~34, pp. 179--182, 1976.

\bibitem{Marton_blowup}
K.~Marton, ``A simple proof of the blowing-up lemma,'' \emph{IEEE Trans. on
  Information Theory}, vol.~32, no.~3, pp. 445--446, May 1986.

\bibitem{Altug_Wagner_arxiv}
\BIBentryALTinterwordspacing
Y.~Altu\v{g} and A.~B. Wagner, ``Refinement of the sphere-packing bound:
  asymmetric channels,'' 2012. [Online]. Available:
  \url{http://arxiv.org/abs/1211.6997.}
\BIBentrySTDinterwordspacing

\bibitem{AmraouiMRU}
A.~Amraoui, A.~Montanari, T.~Richardson, and R.~Urbanke, ``Finite-length
  scaling for iteratively decoded {LDPC} ensembles,'' \emph{IEEE Trans. on
  Information Theory}, vol.~55, no.~2, pp. 473--498, February 2009.

\bibitem{NozakiKS}
T.~Nozaki, K.~Kasai, and K.~Sakaniwa, ``Analytical solution of covariance
  evolution for irregular {LDPC} codes,'' \emph{IEEE Trans. on Information
  Theory}, vol.~58, no.~7, pp. 4770--4780, July 2012.

\bibitem{Kontoyiannis_Verdu}
\BIBentryALTinterwordspacing
I.~Kontoyiannis and S.~Verd\'u, ``Lossless data compression at finite
  blocklengths,'' 2012. [Online]. Available:
  \url{http://arxiv.org/abs/1212.2668.}
\BIBentrySTDinterwordspacing

\bibitem{Kostina_Verdu_IT2012}
V.~Kostina and S.~Verd\'u, ``Fixed-length lossy compression in the finite
  blocklength regime,'' \emph{IEEE Trans. on Information Theory}, vol.~58,
  no.~6, pp. 3309--3338, June 2012.

\bibitem{Matthews_2012}
W.~Matthews, ``A linear program for the finite block length converse of
  {P}olyanskiy-{P}oor-{V}erd\'u via nonsignaling codes,'' \emph{IEEE Trans. on
  Information Theory}, no.~12, pp. 7036--7044, December 2012.

\bibitem{Polyanskiy_Poor_Verdu_IT2010}
Y.~Polyanskiy, H.~V. Poor, and S.~Verd{\'{u}}, ``Channel coding rate in finite
  blocklength regime,'' \emph{IEEE Trans. on Information Theory}, vol.~56,
  no.~5, pp. 2307--2359, May 2010.

\bibitem{Wiechman_Sason_ISP}
G.~Wiechman and I.~Sason, ``An improved sphere-packing bound for finite-length
  codes on symmetric channels,'' \emph{IEEE Trans. on Information Theory},
  vol.~54, no.~5, pp. 1962--1990, 2008.

\bibitem{Rosenthal_book}
J.~S. Rosenthal, \emph{A First Look at Rigorous Probability Theory},
  2nd~ed.\hskip 1em plus 0.5em minus 0.4em\relax World Scientific, 2006.

\bibitem{McDiarmid_bounded_differences_Martingales_1989}
C.~McDiarmid, ``On the method of bounded differences,'' in \emph{Surveys in
  Combinatorics}.\hskip 1em plus 0.5em minus 0.4em\relax Cambridge University
  Press, 1989, vol. 141, pp. 148--188.

\bibitem{KS_1998}
M.~J. Kearns and L.~K. Saul, ``Large deviation methods for approximate
  probabilistic inference,'' in \emph{Proceedings of the 14th Conference on
  Uncertaintly in Artifical Intelligence}, San-Francisco, CA, USA, March~16-18
  1998, pp. 311--319.

\bibitem{Berend_Kontorovich_missing_mass_2012}
D.~Berend and A.~Kontorovich, ``On the concentration of the missing mass,''
  \emph{Electronic Communications in Probability}, vol.~18, no.~3, pp. 1--7,
  January 2013.

\bibitem{From_and_Swift_2011}
S.~G. From and A.~W. Swift, ``A refinement of {H}oeffding's inequality,''
  \emph{Journal of Statistical Computation and Simulation}, pp. 1--7, December
  2011.

\bibitem{Dembo_Zeitouni}
A.~Dembo and O.~Zeitouni, \emph{Large Deviations Techniques and Applications},
  2nd~ed.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 1997.

\bibitem{Billingsley}
P.~Billingsley, \emph{Probability and Measure}, 3rd~ed.\hskip 1em plus 0.5em
  minus 0.4em\relax Wiley Series in Probability and Mathematical Statistics,
  1995.

\bibitem{GrimmettS_book}
G.~Grimmett and D.~Stirzaker, \emph{Probability and Random Processes},
  3rd~ed.\hskip 1em plus 0.5em minus 0.4em\relax Oxford University Press, 2001.

\bibitem{Kontoyiannis_ISIT05}
I.~Kontoyiannis, L.~A. Latras-Montano, and S.~P. Meyn, ``Relative entropy and
  exponential deviation bounds for general {M}arkov chains,'' in
  \emph{Proceedings of the 2005 IEEE International Symposium on Information
  Theory}, Adelaide, Australia, September 2005, pp. 1563--1567.

\bibitem{BargF_IT2002}
A.~Barg and G.~D. Forney, ``Random codes: minimum distances and error
  exponents,'' \emph{IEEE Trans. on Information Theory}, vol.~48, no.~9, pp.
  2568--2573, September 2002.

\bibitem{Breiling04}
M.~Breiling, ``A logarithmic upper bound on the minimum distance of turbo
  codes,'' \emph{IEEE Trans. on Information Theory}, vol.~50, no.~8, pp.
  1692--1710, August 2004.

\bibitem{Gallager_1962}
R.~G. Gallager, ``Low-{D}ensity {P}arity-{C}heck {C}odes,'' Ph.D. dissertation,
  MIT, Cambridge, MA, USA, 1963.

\bibitem{Sason_LDPC09}
I.~Sason, ``On universal properties of capacity-approaching {LDPC} code
  ensembles,'' \emph{IEEE Trans. on Information Theory}, vol.~55, no.~7, pp.
  2956--2990, July 2009.

\bibitem{Cycle_free_codes}
T.~Etzion, A.~Trachtenberg, and A.~Vardy, ``Which codes have cycle-free
  {T}anner graphs?'' \emph{IEEE Trans. on Information Theory}, vol.~45, no.~6,
  pp. 2173--2181, September 1999.

\bibitem{LubyMSS_IT01_paper2}
M.~G. Luby, Mitzenmacher, M.~A. Shokrollahi, and D.~A. Spielmann, ``Improved
  low-density parity-check codes using irregular graphs,'' \emph{IEEE Trans. on
  Information Theory}, vol.~47, no.~2, pp. 585--598, February 2001.

\bibitem{KavcicMM_IT2003}
A.~Kav{\v{c}}i{\'c}, X.~Ma, and M.~Mitzenmacher, ``Binary intersymbol
  interference channels: {G}allager bounds, density evolution, and code
  performance bounds,'' \emph{IEEE Trans. on Information Theory}, vol.~49,
  no.~7, pp. 1636--1652, July 2003.

\bibitem{Eshel_MSc2011}
R.~Eshel, ``Aspects of {C}onvex {O}ptimization and {C}oncentration in
  {C}oding,'' {MSc} thesis, Department of Electrical Engineering, Technion -
  Israel Institute of Technology, Haifa, Israel, February~2012.

\bibitem{Douillard_sept95}
J.~Douillard, M.~Jezequel, C.~Berrou, A.~Picart, P.~Didier, and A.~Glavieux,
  ``Iterative correction of intersymbol interference: turbo-equalization,''
  \emph{Eurpoean Transactions on Telecommunications}, vol.~6, no.~1, pp.
  507--511, September 1995.

\bibitem{MeassonMU08}
C.~M\'{e}asson, A.~Montanari, and R.~Urbanke, ``Maxwell construction: the
  hidden bridge between iterative and maximum apposteriori decoding,''
  \emph{IEEE Trans. on Information Theory}, vol.~54, no.~12, pp. 5277--5307,
  December 2008.

\bibitem{Shokrollahi-IMA2000}
A.~Shokrollahi, ``Capacity-achieving sequences,'' in \emph{Volume in
  Mathematics and its Applications}, vol. 123, 2000, pp. 153--166.

\bibitem{Molisch_book}
A.~F. Molisch, \emph{Wireless Communications}.\hskip 1em plus 0.5em minus
  0.4em\relax John Wiley and Sons, 2005.

\bibitem{WunderFBLN_OFDM_tutorial_2012}
G.~Wunder, R.~F.~H. Fischer, H.~Boche, S.~Litsyn, and J.~S. No, ``The {PAPR}
  problem in {OFDM} transmission: new directions for a long-lasting problem,''
  accepted to the {\em {IEEE} {S}ignal {P}rocessing {M}agazine}, {D}ecember
  2012. [Online]. Available: http://arxiv.org/abs/1212.2865.

\bibitem{LitsynW06}
S.~Litsyn and G.~Wunder, ``Generalized bounds on the crest-factor istribution
  of {OFDM} signals with applications to code design,'' \emph{IEEE Trans. on
  Information Theory}, vol.~52, no.~3, pp. 992--1006, March 2006.

\bibitem{SalemZ}
R.~Salem and A.~Zygmund, ``Some properties of trigonometric series whose terms
  have random signs,'' \emph{Acta Mathematica}, vol.~91, no.~1, pp. 245--301,
  1954.

\bibitem{WunderB_IT}
G.~Wunder and H.~Boche, ``New results on the statistical distribution of the
  crest-factor of {OFDM} signals,'' \emph{IEEE Trans. on Information Theory},
  vol.~49, no.~2, pp. 488--494, February 2003.

\bibitem{Benedetto_Biglieri_book}
S.~Benedetto and E.~Biglieri, \emph{Principles of Digital Transmission with
  Wireless Applications}.\hskip 1em plus 0.5em minus 0.4em\relax Kluwer
  Academic/ Plenum Publishers, 1999.

\bibitem[Boucheron et~al.(2005)Boucheron, Bousquet, and Lugosi]{BBL04b}
S.\ Boucheron, O.\ Bousquet, and G.\ Lugosi.
\newblock Theory of classification: a survey of recent advances.
\newblock \emph{ESAIM: Probability and Statistics}, 9:\penalty0 323--375, 2005.
\bibitem[Gine et all]{Gine1984}
\newblock Some Limit theorems for Empirical Processes 
\newblock \emph{Ann. Probab.} Volume 12, Number 4 (1984), 929-989

\end{thebibliography}
\end{document}
